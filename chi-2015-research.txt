TabLETS Get Physical: Non-Visual Text Entry on Tablet Devices

The problem of the often used touchscreen-keyboards is that they have no tactile feedback and no orientation keys F and J. So blind people have problems with the orientation on the keyboard. The solutions for this problem at the momenat are VoiceOver and TalkBack. They resort to an "Explore by Touch" paradigm where users browse the screen content by moving a single point around the interface that eads aloud the element in focus. But this way of text entry is very slow for blind people. So they deveoloped "SpatialTouch", which is an input system for blind users that leverages previous experience on physical QWERTY keyboards, by supporting multitouch exploration. The results show that although SpatialTouch did not result in faster input rates overall, it was indeed able to leverage previous QWERTY experience in contrast to Explore by Touch and blind people were able to interact much better with the keyboard.

Investigating the Dexterity of Multi-Finger Input for Mid-Air Text Entry

This paper shows the possibilty of multi-finger input for mid-air text entry by reporting on the speed, accuracy, individuation, movement ranges, and individual differences of each finger. The hand is the most dexterous of the extremities for text entry beacuse of its many degrees of freedom, and fast and precise movements. Additionally with the modern techniques a single camera is enough to capture different movements. The experiements show that there are differences up to 50% in movement times of single fingers. Furthermore, they provide indices quantifying the individuation. Results show that mid-air text input is a promising input modality. But there are also some limitations, e.g. hard learnability, technical challenges for hand tracking (errors at fast motion) and feedback for the user (it is unknown if proprioception alone is enough to perform fast and accurate mid-air gestures).

THING: Introducing a Tablet-based Interaction Technique for Controlling 3D Hand Models

Animating a virtual 3D model of a hand can be very time-consuming and costly using current methods, such as motion tracking. This paper introduces a new approach to quickly animate a 3D hand, which is called THING. THING is a tablet-based approach that leverages multi-touch interaction for a quick and precise control of a 3D hand’s pose. The flexion/extension and abduction/adduction of the virtual fingers can be controlled for each finger individually or for several fingers in parallel through sliding motions on the tablet’s surface. They designed two variants of their prototype, MobileThing, which maps the spatial location and orientation of the tablet to that of the virtual hand, and DesktopTHING, which combines multi-touch controls of fingers with traditional mouse controls for the hand’s global position and orientation. Furthermore they compared the usability of THING against mouse-only and data glove controls. Results show that DesktopTHING provided performance similar to that of a data glove.

